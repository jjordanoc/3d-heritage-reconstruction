Due to the lack of good metrics for keyframe selection, we are forced to use an indirect method of assesment for the quality of our keyframe selection strategies [Keyframe Selection for Visual Localization and Mapping
Tasks: A Systematic Literature Review]. The proposed method for evaluation consists in the execution of the model over the full dataset and the selected keyframes, and the score is given as a point cloud similarity metric between this two. The intention is to capture how well the keyframing preserved structure information that the model was then able to retrieve. The key assumption underpinning this methodology is that the model can extract at least all the information from the best possible selection of keyframes using the entire dataset and hence a run using the full dataset is a good proxy for ground truth.
However, due to the limitations of the model in regards to scale, rotation and displacement, a previous preprocessing of the generated pointclouds is needed. This process is liable to introduce some error to the measurements, and as so this metric is flawed. Nevertheless, we consider it to be sufficient to compare methods as all methods are equally subject to the same source of error.
We made three experiments evaluating a specific method for keyframe selection. The experiments consisted of a run of the keyframe selection algorithm over the entire clean dataset of size N = 71, selecting ceil(5log(N)) = 31 images as keyframes with each method. Then, a full inference was run on the resulting keyframes wich returns a pointcloud that was registered with a reference pointcloud resulting of inference over the full dataset. Finally, the registered pointcloud's chamfer's distance is reported.
We tested the the MCNF method described in [A Unified View-Graph Selection Framework for Structure from Motion] wich finds the subset of keyframes by first building a view graph, and then finding the subgraph that keeps the pairs which maximise the feature matches. As this method selects edges in the view graph and not images, we had to experimentally tweak the min-flow parameter to log(N) to produce a selection of appropiate size. Notably the execution time of this method is high due to the need to build the computationally expensive view graph.
As a second option we evaluated the method described by [Real-Time Visual SLAM for Autonomous Underwater Hull Inspection using Visual Saliency], wich selects keyframes based on saliency. It finds and scores features in the image and on the global dataset to compute a measure of how unexpected the image is for the dataset. An interesting property of the method is that it is online, wich yields itself to our purposes particularly well.
Finally we attempted an ad-hoc method wich instead of relying on hand crafted features such as SIFT or ORB directly uses Resnet50 embeddings from the second-to-last layer to construct a similarity graph reminiscent of the MCNF described above. Then, cosine similarity is used as a weight for a fully connected graph. All edges with similarities above a threshold T (for tests T = 0.85) are removed and then edges and their asociated images are popped from the graph in descending weight order until the required number of keyframes is reached. Note that the used weights are the result of training in the ImageNet1k dataset, as is the default for pytorch.
Thirdly, as a benchmark we evaluated a random choice from the dataset wich should serve as a benchmark.
For each of the keyframe selection methods we select from a dataset of N images log(N) keyframes. We then compare the pointclouds resulting from using PI3 on this dataset using chamfer's distance and show the results in table:

Random -   0.0019496411033363273
MCNF -     0.002566248385559148
Saliency - 0.033318889272846555
Resnet -   0.0010482053260287667