{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b433b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/root/jupyter/monst3r' already exists and is not an empty directory.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/utils/_process_posix.py:151\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    152\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit clone --recursive https://github.com/junyi42/monst3r /root/jupyter/monst3r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcd monst3r\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ipykernel/zmqshell.py:685\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/utils/_process_posix.py:167\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/junyi42/monst3r /root/jupyter/monst3r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4cf4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/__modal/volumes/vo-05VlmA5Lclgm1HklUu8tDq/monst3r/croco/models/curope\n",
      "running build_ext\n",
      "W1014 17:24:08.508000 30 site-packages/torch/utils/cpp_extension.py:615] Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "W1014 17:24:09.083000 30 site-packages/torch/utils/cpp_extension.py:507] The detected CUDA version (12.1) has a minor version mismatch with the version that was used to compile PyTorch (12.8). Most likely this shouldn't be a problem.\n",
      "W1014 17:24:09.084000 30 site-packages/torch/utils/cpp_extension.py:517] There are no clang++ version bounds defined for CUDA version 12.1\n",
      "building 'curope' extension\n",
      "creating build\n",
      "creating build/temp.linux-x86_64-cpython-310\n",
      "clang -pthread -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/tools/deps/include -I/tools/deps/include/ncursesw -I/tools/deps/libedit/include -g0 -fPIC -I/usr/local/lib/python3.10/site-packages/torch/include -I/usr/local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/local/include/python3.10 -c curope.cpp -o build/temp.linux-x86_64-cpython-310/curope.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -DTORCH_EXTENSION_NAME=curope -std=c++17\n",
      "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/site-packages/torch/include -I/usr/local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/local/include/python3.10 -c kernels.cu -o build/temp.linux-x86_64-cpython-310/kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --ptxas-options=-v --use_fast_math -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -DTORCH_EXTENSION_NAME=curope -std=c++17\n",
      "nvcc fatal   : Unsupported gpu architecture 'compute_100'\n",
      "error: command '/usr/local/cuda/bin/nvcc' failed with exit code 1\n"
     ]
    }
   ],
   "source": [
    "# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.\n",
    "%cd /root/jupyter/monst3r/croco/models/curope/\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc25a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3896357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3D_model_from_scene(outdir, silent, scene, min_conf_thr=3, as_pointcloud=False, mask_sky=False,\n",
    "                            clean_depth=False, transparent_cams=False, cam_size=0.05, show_cam=True, save_name=None, thr_for_init_conf=True):\n",
    "    \"\"\"\n",
    "    extract 3D_model (glb file) from a reconstructed scene\n",
    "    \"\"\"\n",
    "    if scene is None:\n",
    "        return None\n",
    "    # post processes\n",
    "    if clean_depth:\n",
    "        scene = scene.clean_pointcloud()\n",
    "    if mask_sky:\n",
    "        scene = scene.mask_sky()\n",
    "\n",
    "    # get optimized values from scene\n",
    "    rgbimg = scene.imgs\n",
    "    focals = scene.get_focals().cpu()\n",
    "    cams2world = scene.get_im_poses().cpu()\n",
    "    # 3D pointcloud from depthmap, poses and intrinsics\n",
    "    pts3d = to_numpy(scene.get_pts3d(raw_pts=True))\n",
    "    scene.min_conf_thr = min_conf_thr\n",
    "    scene.thr_for_init_conf = thr_for_init_conf\n",
    "    msk = to_numpy(scene.get_masks())\n",
    "    cmap = pl.get_cmap('viridis')\n",
    "    cam_color = [cmap(i/len(rgbimg))[:3] for i in range(len(rgbimg))]\n",
    "    cam_color = [(255*c[0], 255*c[1], 255*c[2]) for c in cam_color]\n",
    "    return convert_scene_output_to_glb(outdir, rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size, show_cam=show_cam, silent=silent, save_name=save_name,\n",
    "                                        cam_color=cam_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reconstructed_scene(args, outdir, model, device, silent, image_size, schedule, niter, min_conf_thr,\n",
    "                            as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size, show_cam, scenegraph_type, winsize, refid, \n",
    "                            seq_name, new_model_weights, temporal_smoothing_weight, translation_weight, shared_focal, \n",
    "                            flow_loss_weight, flow_loss_start_iter, flow_loss_threshold, use_gt_mask, fps, num_frames):\n",
    "    \"\"\"\n",
    "    from a list of images, run dust3r inference, global aligner.\n",
    "    then run get_3D_model_from_scene\n",
    "    \"\"\"\n",
    "    translation_weight = float(translation_weight)\n",
    "    if new_model_weights != args.weights:\n",
    "        model = AsymmetricCroCo3DStereo.from_pretrained(new_model_weights).to(device)\n",
    "    model.eval()\n",
    "    if seq_name != \"NULL\":\n",
    "        dynamic_mask_path = f'data/davis/DAVIS/masked_images/480p/{seq_name}'\n",
    "    else:\n",
    "        dynamic_mask_path = None\n",
    "\n",
    "    imgs = torch.load(\"/root/jupyter/data/preprocessed_data.pth\")\n",
    "        \n",
    "    if len(imgs) == 1:\n",
    "        imgs = [imgs[0], copy.deepcopy(imgs[0])]\n",
    "        imgs[1]['idx'] = 1\n",
    "    if scenegraph_type == \"swin\" or scenegraph_type == \"swinstride\" or scenegraph_type == \"swin2stride\":\n",
    "        scenegraph_type = scenegraph_type + \"-\" + str(winsize) + \"-noncyclic\"\n",
    "    elif scenegraph_type == \"oneref\":\n",
    "        scenegraph_type = scenegraph_type + \"-\" + str(refid)\n",
    "\n",
    "    pairs = make_pairs(imgs, scene_graph=scenegraph_type, prefilter=None, symmetrize=True)\n",
    "    output = inference(pairs, model, device, batch_size=args.batch_size, verbose=not silent)\n",
    "    # TODO YYJ del model\n",
    "    if len(imgs) > 2:\n",
    "        mode = GlobalAlignerMode.PointCloudOptimizer\n",
    "        scene = global_aligner(output, device=device, mode=mode, verbose=not silent, shared_focal = shared_focal, temporal_smoothing_weight=temporal_smoothing_weight, translation_weight=translation_weight,\n",
    "                               flow_loss_weight=flow_loss_weight, flow_loss_start_epoch=flow_loss_start_iter, flow_loss_thre=flow_loss_threshold, use_self_mask=not use_gt_mask,\n",
    "                               num_total_iter=niter, empty_cache= len(filelist) > 72, batchify=not (args.not_batchify or args.window_wise),\n",
    "                               window_wise=args.window_wise, window_size=args.window_size, window_overlap_ratio=args.window_overlap_ratio,\n",
    "                               prev_video_results=prev_video_results)\n",
    "    else:\n",
    "        mode = GlobalAlignerMode.PairViewer\n",
    "        scene = global_aligner(output, device=device, mode=mode, verbose=not silent)\n",
    "    lr = 0.01\n",
    "\n",
    "    if mode == GlobalAlignerMode.PointCloudOptimizer:\n",
    "        if args.window_wise:\n",
    "            scene.compute_window_wise_alignment(init='mst', niter=niter, schedule=schedule, lr=lr)\n",
    "        else:\n",
    "            scene.compute_global_alignment(init='mst', niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "    if args.window_wise and args.prev_output_dir is not None:\n",
    "        scene.clean_prev_results()\n",
    "        \n",
    "    save_folder = f'{args.output_dir}/{seq_name}'  #default is 'demo_tmp/NULL'\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    outfile = get_3D_model_from_scene(save_folder, silent, scene, min_conf_thr, as_pointcloud, mask_sky,\n",
    "                            clean_depth, transparent_cams, cam_size, show_cam)\n",
    "\n",
    "    poses = scene.save_tum_poses(f'{save_folder}/pred_traj.txt')\n",
    "    K = scene.save_intrinsics(f'{save_folder}/pred_intrinsics.txt')\n",
    "    depth_maps = scene.save_depth_maps(save_folder)\n",
    "    dynamic_masks = scene.save_dynamic_masks(save_folder)\n",
    "    conf = scene.save_conf_maps(save_folder)\n",
    "    init_conf = scene.save_init_conf_maps(save_folder)\n",
    "    rgbs = scene.save_rgb_imgs(save_folder)\n",
    "    enlarge_seg_masks(save_folder, kernel_size=5 if use_gt_mask else 3) \n",
    "\n",
    "    # also return rgb, depth and confidence imgs\n",
    "    # depth is normalized with the max value for all images\n",
    "    # we apply the jet colormap on the confidence maps\n",
    "    rgbimg = scene.imgs\n",
    "    depths = to_numpy(scene.get_depthmaps())\n",
    "    confs = to_numpy([c for c in scene.im_conf])\n",
    "    init_confs = to_numpy([c for c in scene.init_conf_maps])\n",
    "    cmap = pl.get_cmap('jet')\n",
    "    depths_max = max([d.max() for d in depths])\n",
    "    depths = [cmap(d/depths_max) for d in depths]\n",
    "    confs_max = max([d.max() for d in confs])\n",
    "    confs = [cmap(d/confs_max) for d in confs]\n",
    "    init_confs_max = max([d.max() for d in init_confs])\n",
    "    init_confs = [cmap(d/init_confs_max) for d in init_confs]\n",
    "\n",
    "    imgs = []\n",
    "    for i in range(len(rgbimg)):\n",
    "        imgs.append(rgbimg[i])\n",
    "        imgs.append(rgb(depths[i]))\n",
    "        imgs.append(rgb(confs[i]))\n",
    "        imgs.append(rgb(init_confs[i]))\n",
    "\n",
    "    # if two images, and the shape is same, we can compute the dynamic mask\n",
    "    if len(rgbimg) == 2 and rgbimg[0].shape == rgbimg[1].shape:\n",
    "        motion_mask_thre = 0.35\n",
    "        error_map = get_dynamic_mask_from_pairviewer(scene, both_directions=True, output_dir=args.output_dir, motion_mask_thre=motion_mask_thre)\n",
    "        # imgs.append(rgb(error_map))\n",
    "        # apply threshold on the error map\n",
    "        normalized_error_map = (error_map - error_map.min()) / (error_map.max() - error_map.min())\n",
    "        error_map_max = normalized_error_map.max()\n",
    "        error_map = cmap(normalized_error_map/error_map_max)\n",
    "        imgs.append(rgb(error_map))\n",
    "        binary_error_map = (normalized_error_map > motion_mask_thre).astype(np.uint8)\n",
    "        imgs.append(rgb(binary_error_map*255))\n",
    "\n",
    "    return scene, outfile, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60e2de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i \"s/torch.load(model_path, map_location='cpu')/torch.load(model_path, map_location='cpu', weights_only=False)/\" /root/jupyter/monst3r/dust3r/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c8e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "torch.serialization.add_safe_globals([argparse.Namespace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f88a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/__modal/volumes/vo-05VlmA5Lclgm1HklUu8tDq/monst3r\n",
      "Using device: cuda\n",
      "Outputting results in: /root/jupyter/output\n",
      "Loading weights from checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "... loading model from checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "instantiating : AsymmetricCroCo3DStereo(pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, freeze='encoder', landscape_only=False)\n",
      "Freezing encoder parameters\n",
      "<All keys matched successfully>\n",
      "Starting reconstruction...\n",
      "Loading preprocessed data from '/root/jupyter/data/preprocessed_data.pth'\n",
      ">> Inference with model on 27320 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27320/27320 [1:04:35<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Imports and setup\n",
    "# --------------------------------------------------------\n",
    "%cd /root/jupyter/monst3r\n",
    "import sys\n",
    "# Add monst3r directory to Python path to find the modules\n",
    "if 'monst3r' not in sys.path:\n",
    "    sys.path.append('monst3r')\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "from dust3r.inference import inference\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.utils.image import load_images, load_prev_video_results, rgb, enlarge_seg_masks\n",
    "from dust3r.utils.device import to_numpy\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "from dust3r.utils.viz_demo import convert_scene_output_to_glb, get_dynamic_mask_from_pairviewer\n",
    "import matplotlib.pyplot as pl\n",
    "import cv2\n",
    "\n",
    "pl.ion()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Function definitions from the Gradio demo script\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def get_3D_model_from_scene(outdir, silent, scene, min_conf_thr=3, as_pointcloud=False, mask_sky=False,\n",
    "                            clean_depth=False, transparent_cams=False, cam_size=0.05, show_cam=True, save_name=None, thr_for_init_conf=True):\n",
    "    \"\"\"\n",
    "    extract 3D_model (glb file) from a reconstructed scene\n",
    "    \"\"\"\n",
    "    if scene is None:\n",
    "        return None\n",
    "    # post processes\n",
    "    if clean_depth:\n",
    "        scene = scene.clean_pointcloud()\n",
    "    if mask_sky:\n",
    "        scene = scene.mask_sky()\n",
    "\n",
    "    # get optimized values from scene\n",
    "    rgbimg = scene.imgs\n",
    "    focals = scene.get_focals().cpu()\n",
    "    cams2world = scene.get_im_poses().cpu()\n",
    "    # 3D pointcloud from depthmap, poses and intrinsics\n",
    "    pts3d = to_numpy(scene.get_pts3d(raw_pts=True))\n",
    "    scene.min_conf_thr = min_conf_thr\n",
    "    scene.thr_for_init_conf = thr_for_init_conf\n",
    "    msk = to_numpy(scene.get_masks())\n",
    "    cmap = pl.get_cmap('viridis')\n",
    "    cam_color = [cmap(i/len(rgbimg))[:3] for i in range(len(rgbimg))]\n",
    "    cam_color = [(255*c[0], 255*c[1], 255*c[2]) for c in cam_color]\n",
    "    return convert_scene_output_to_glb(outdir, rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size, show_cam=show_cam, silent=silent, save_name=save_name,\n",
    "                                        cam_color=cam_color)\n",
    "\n",
    "def get_reconstructed_scene(args, outdir, model, device, silent, image_size, schedule, niter, min_conf_thr,\n",
    "                            as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size, show_cam, scenegraph_type, winsize, refid, \n",
    "                            seq_name, new_model_weights, temporal_smoothing_weight, translation_weight, shared_focal, \n",
    "                            flow_loss_weight, flow_loss_start_iter, flow_loss_threshold, use_gt_mask, fps, num_frames):\n",
    "    \"\"\"\n",
    "    from a list of images, run dust3r inference, global aligner.\n",
    "    then run get_3D_model_from_scene\n",
    "    \"\"\"\n",
    "    translation_weight = float(translation_weight)\n",
    "    if new_model_weights != args.weights:\n",
    "        model = AsymmetricCroCo3DStereo.from_pretrained(new_model_weights).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # <<< YOUR MODIFICATION IS HERE\n",
    "    # Using your specified method to load preprocessed data\n",
    "    print(\"Loading preprocessed data from '/root/jupyter/data/preprocessed_data.pth'\")\n",
    "    imgs = torch.load(\"/root/jupyter/data/preprocessed_data.pth\", weights_only=False)\n",
    "    # >>> END OF MODIFICATION\n",
    "        \n",
    "    if len(imgs) == 1:\n",
    "        imgs = [imgs[0], copy.deepcopy(imgs[0])]\n",
    "        imgs[1]['idx'] = 1\n",
    "    if scenegraph_type == \"swin\" or scenegraph_type == \"swinstride\" or scenegraph_type == \"swin2stride\":\n",
    "        scenegraph_type = scenegraph_type + \"-\" + str(winsize) + \"-noncyclic\"\n",
    "    elif scenegraph_type == \"oneref\":\n",
    "        scenegraph_type = scenegraph_type + \"-\" + str(refid)\n",
    "\n",
    "    pairs = make_pairs(imgs, scene_graph=scenegraph_type, prefilter=None, symmetrize=True)\n",
    "    output = inference(pairs, model, device, batch_size=args.batch_size, verbose=not silent)\n",
    "    \n",
    "    # The rest of the function remains the same...\n",
    "    if len(imgs) > 2:\n",
    "        mode = GlobalAlignerMode.PointCloudOptimizer\n",
    "        scene = global_aligner(output, device=device, mode=mode, verbose=not silent, shared_focal = shared_focal, temporal_smoothing_weight=temporal_smoothing_weight, translation_weight=translation_weight,\n",
    "                               flow_loss_weight=flow_loss_weight, flow_loss_start_epoch=flow_loss_start_iter, flow_loss_thre=flow_loss_threshold, use_self_mask=not use_gt_mask,\n",
    "                               num_total_iter=niter, empty_cache= len(imgs) > 72, batchify=not (args.not_batchify or args.window_wise),\n",
    "                               window_wise=args.window_wise, window_size=args.window_size, window_overlap_ratio=args.window_overlap_ratio,\n",
    "                               prev_video_results=None) # prev_video_results is not used with preprocessed data\n",
    "    else:\n",
    "        mode = GlobalAlignerMode.PairViewer\n",
    "        scene = global_aligner(output, device=device, mode=mode, verbose=not silent)\n",
    "    lr = 0.01\n",
    "\n",
    "    if mode == GlobalAlignerMode.PointCloudOptimizer:\n",
    "        if args.window_wise:\n",
    "            scene.compute_window_wise_alignment(init='mst', niter=niter, schedule=schedule, lr=lr)\n",
    "        else:\n",
    "            scene.compute_global_alignment(init='mst', niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "    if args.window_wise and args.prev_output_dir is not None:\n",
    "        scene.clean_prev_results()\n",
    "        \n",
    "    save_folder = f'{args.output_dir}/{seq_name}'\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    outfile = get_3D_model_from_scene(save_folder, silent, scene, min_conf_thr, as_pointcloud, mask_sky,\n",
    "                            clean_depth, transparent_cams, cam_size, show_cam)\n",
    "\n",
    "    poses = scene.save_tum_poses(f'{save_folder}/pred_traj.txt')\n",
    "    K = scene.save_intrinsics(f'{save_folder}/pred_intrinsics.txt')\n",
    "    depth_maps = scene.save_depth_maps(save_folder)\n",
    "    dynamic_masks = scene.save_dynamic_masks(save_folder)\n",
    "    conf = scene.save_conf_maps(save_folder)\n",
    "    init_conf = scene.save_init_conf_maps(save_folder)\n",
    "    rgbs = scene.save_rgb_imgs(save_folder)\n",
    "    enlarge_seg_masks(save_folder, kernel_size=5 if use_gt_mask else 3) \n",
    "\n",
    "    rgbimg = scene.imgs\n",
    "    depths = to_numpy(scene.get_depthmaps())\n",
    "    confs = to_numpy([c for c in scene.im_conf])\n",
    "    init_confs = to_numpy([c for c in scene.init_conf_maps])\n",
    "    \n",
    "    imgs_for_viz = []\n",
    "\n",
    "    return scene, outfile, imgs_for_viz\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Main execution logic for the notebook\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# 1. Setup arguments (instead of argparse)\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # You may need to download the weights first.\n",
    "        # The default path is from the original script.\n",
    "        self.weights = 'checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth'\n",
    "        self.model_name = 'Junyi42/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.output_dir = '/root/jupyter/output'\n",
    "        self.prev_output_dir = None\n",
    "        self.prev_output_index = None\n",
    "        self.silent = False\n",
    "        # # --- IMPORTANT: Change this to your image folder or video file ---\n",
    "        # self.input_dir = '/root/jupy' \n",
    "        self.seq_name = 'huaca' # This will be the output subfolder name\n",
    "        self.use_gt_davis_masks = False\n",
    "        self.not_batchify = False\n",
    "        self.real_time = False\n",
    "        self.window_wise = False\n",
    "        self.window_size = 100\n",
    "        self.window_overlap_ratio = 0.5\n",
    "        self.batch_size = 100 # Lower this if you get CUDA out-of-memory errors\n",
    "        self.fps = 0\n",
    "        self.num_frames = 200\n",
    "        self.image_size = 512\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Create output directory\n",
    "tmpdirname = args.output_dir\n",
    "os.makedirs(tmpdirname, exist_ok=True)\n",
    "tempfile.tempdir = tmpdirname\n",
    "\n",
    "print(f'Using device: {args.device}')\n",
    "print('Outputting results in:', tmpdirname)\n",
    "\n",
    "# 2. Load model\n",
    "if args.weights and os.path.exists(args.weights):\n",
    "    weights_path = args.weights\n",
    "    print(f\"Loading weights from {weights_path}\")\n",
    "else:\n",
    "    print(f\"Weights file not found at '{args.weights}'. Attempting to load from Hugging Face Hub model name '{args.model_name}'.\")\n",
    "    weights_path = args.model_name\n",
    "\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(weights_path).to(args.device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 4. Run reconstruction\n",
    "print(\"Starting reconstruction...\")\n",
    "scene, outfile, imgs = get_reconstructed_scene(\n",
    "    args=args,\n",
    "    outdir=tmpdirname,\n",
    "    model=model,\n",
    "    device=args.device,\n",
    "    silent=args.silent,\n",
    "    image_size=args.image_size,\n",
    "    # filelist=input_files,\n",
    "    schedule='linear',\n",
    "    niter=300,\n",
    "    min_conf_thr=1.1,\n",
    "    as_pointcloud=True,\n",
    "    mask_sky=False,\n",
    "    clean_depth=True,\n",
    "    transparent_cams=False,\n",
    "    cam_size=0.05,\n",
    "    show_cam=True,\n",
    "    scenegraph_type='swinstride',\n",
    "    winsize=5,\n",
    "    refid=0,\n",
    "    seq_name=args.seq_name,\n",
    "    new_model_weights=args.weights,\n",
    "    temporal_smoothing_weight=0.01,\n",
    "    translation_weight='1.0',\n",
    "    shared_focal=True,\n",
    "    flow_loss_weight=0.01,\n",
    "    flow_loss_start_iter=0.1,\n",
    "    flow_loss_threshold=25,\n",
    "    use_gt_mask=args.use_gt_davis_masks,\n",
    "    fps=args.fps,\n",
    "    num_frames=args.num_frames,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing completed.\")\n",
    "print(f\"Output maps and data saved in: {os.path.join(tmpdirname, args.seq_name)}\")\n",
    "if outfile:\n",
    "    print(f\"3D model saved to: {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117839d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_bonn.sh             download_spring.sh\n",
      "download_ckpt.sh             download_tartanair.py\n",
      "download_davis.py            download_tartanair.sh\n",
      "download_dynamic_replica.sh  download_training_zipfiles.txt\n",
      "download_kitti.sh            download_tum_dynamics.sh\n",
      "download_nyuv2.sh            download_waymo.sh\n",
      "download_pointodyssey.sh     evaluation_script.md\n",
      "download_scannetv2.sh        prepare_training.md\n",
      "download_sintel.sh\n"
     ]
    }
   ],
   "source": [
    "%ls /root/jupyter/monst3r/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3159e5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/__modal/volumes/vo-05VlmA5Lclgm1HklUu8tDq/monst3r/data\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Z1jO_JmfZj0z3bgMvCwqfUhyZ1bIbc9E\n",
      "From (redirected): https://drive.google.com/uc?id=1Z1jO_JmfZj0z3bgMvCwqfUhyZ1bIbc9E&confirm=t&uuid=12fabc9d-a0ec-40a2-adfa-0017fd7efa7c\n",
      "To: /__modal/volumes/vo-05VlmA5Lclgm1HklUu8tDq/monst3r/checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "100%|██████████████████████████████████████| 2.29G/2.29G [00:28<00:00, 81.2MB/s]\n",
      "--2025-10-14 17:33:27--  https://www.dropbox.com/s/4j4z58wuv8o0mfz/models.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.11.18, 2620:100:6050:18::a27d:b12\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.11.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/1gtoiul6tr71ag8xjet5v/models.zip?rlkey=lu59i7g742lo6o2yt15f8t96r [following]\n",
      "--2025-10-14 17:33:28--  https://www.dropbox.com/scl/fi/1gtoiul6tr71ag8xjet5v/models.zip?rlkey=lu59i7g742lo6o2yt15f8t96r\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com/cd/0/inline/CzNanDkioYeyqFLayLphtccQ-r52b_OZuPQrwEVtYs_S52nI3jYS6M6RdOSRVZDpiM5-xofU9aCw8ApDNccQ-EnJ0XfWR_zFo1utggsfijlWlwxCW_PdfKtmlfHatQVqgT1kKqEc11kKV_t7jYGDlmvg/file# [following]\n",
      "--2025-10-14 17:33:28--  https://uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com/cd/0/inline/CzNanDkioYeyqFLayLphtccQ-r52b_OZuPQrwEVtYs_S52nI3jYS6M6RdOSRVZDpiM5-xofU9aCw8ApDNccQ-EnJ0XfWR_zFo1utggsfijlWlwxCW_PdfKtmlfHatQVqgT1kKqEc11kKV_t7jYGDlmvg/file\n",
      "Resolving uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com (uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com)... 162.125.11.15, 2620:100:6050:15::a27d:b0f\n",
      "Connecting to uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com (uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com)|162.125.11.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/CzPUYSJOcjGj0vo4CZO5JXgBZnNNrN0cUQiv6m0aYzfyqYo-hFM9uEd52wzGqKgcTmxs7JQqXCx5bilY5tb99T7mwaRZ3VQUoX06WVQ0yKXS_D12Onh_14prR9bjsxuwDUfKu8RNyYTwFdMSnu285lOvpVkKcCMgFJRt8IYyWcTh0fyOehFMPIMDdNE_L2TYl5dmkuc5aEf26w4zX-7h9t2iPMY7z2TCQcYc_8pP0MZqBTOqO4vP1g4mn043R5TfMFjmwNWCH1H7OPCKlQuGUFri-ZeqjIhj8t0a6U_zDeEsuGmtbAuBjwMBP1HB5CX6YKToy7swLXyTEtn3apaPMdcIbfag9bjvO9lIkMxnkF4guxvaXsszH_IjPzs04JXYp04/file [following]\n",
      "--2025-10-14 17:33:29--  https://uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com/cd/0/inline2/CzPUYSJOcjGj0vo4CZO5JXgBZnNNrN0cUQiv6m0aYzfyqYo-hFM9uEd52wzGqKgcTmxs7JQqXCx5bilY5tb99T7mwaRZ3VQUoX06WVQ0yKXS_D12Onh_14prR9bjsxuwDUfKu8RNyYTwFdMSnu285lOvpVkKcCMgFJRt8IYyWcTh0fyOehFMPIMDdNE_L2TYl5dmkuc5aEf26w4zX-7h9t2iPMY7z2TCQcYc_8pP0MZqBTOqO4vP1g4mn043R5TfMFjmwNWCH1H7OPCKlQuGUFri-ZeqjIhj8t0a6U_zDeEsuGmtbAuBjwMBP1HB5CX6YKToy7swLXyTEtn3apaPMdcIbfag9bjvO9lIkMxnkF4guxvaXsszH_IjPzs04JXYp04/file\n",
      "Reusing existing connection to uc38665a6547f99fd4b0bbfd718d.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81977417 (78M) [application/zip]\n",
      "Saving to: ‘models.zip’\n",
      "\n",
      "models.zip          100%[===================>]  78.18M  29.3MB/s    in 2.7s    \n",
      "\n",
      "2025-10-14 17:33:32 (29.3 MB/s) - ‘models.zip’ saved [81977417/81977417]\n",
      "\n",
      "Archive:  models.zip\n",
      "   creating: models/\n",
      "  inflating: models/raft-kitti.pth   \n",
      "  inflating: models/raft-sintel.pth  \n",
      "  inflating: models/raft-chairs.pth  \n",
      "  inflating: models/raft-things.pth  \n",
      "  inflating: models/raft-small.pth   \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1a0C5FTdhjM4rKrfXiGhec7eq2YM141lu\n",
      "From (redirected): https://drive.google.com/uc?id=1a0C5FTdhjM4rKrfXiGhec7eq2YM141lu&confirm=t&uuid=7d8da83a-766c-4f43-bbf8-0b331677221a\n",
      "To: /__modal/volumes/vo-05VlmA5Lclgm1HklUu8tDq/monst3r/third_party/RAFT/models/Tartan-C-T-TSKH-spring540x960-M.pth\n",
      "100%|██████████████████████████████████████| 78.9M/78.9M [00:00<00:00, 81.4MB/s]\n",
      "--2025-10-14 17:33:36--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.160.22.125, 3.160.22.85, 3.160.22.80, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.160.22.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
      "Saving to: ‘checkpoints/sam2.1_hiera_large.pt’\n",
      "\n",
      "sam2.1_hiera_large. 100%[===================>] 856.48M  19.6MB/s    in 45s     \n",
      "\n",
      "2025-10-14 17:34:21 (19.2 MB/s) - ‘checkpoints/sam2.1_hiera_large.pt’ saved [898083611/898083611]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /root/jupyter/monst3r/data\n",
    "!bash download_ckpt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0281a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_bonn.sh             download_spring.sh\n",
      "download_ckpt.sh             download_tartanair.py\n",
      "download_davis.py            download_tartanair.sh\n",
      "download_dynamic_replica.sh  download_training_zipfiles.txt\n",
      "download_kitti.sh            download_tum_dynamics.sh\n",
      "download_nyuv2.sh            download_waymo.sh\n",
      "download_pointodyssey.sh     evaluation_script.md\n",
      "download_scannetv2.sh        prepare_training.md\n",
      "download_sintel.sh\n"
     ]
    }
   ],
   "source": [
    "%ls /root/jupyter/monst3r/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c268fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCUT3R\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34mmonst3r\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls /root/jupyter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633855e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
